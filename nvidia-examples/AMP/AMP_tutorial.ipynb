{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AMP (Automatic Mixed Precision) in MXNet\n",
    "\n",
    "Training Deep Learning networks is a very computationally intensive task. Novel model architectures tend to have increasing number of layers and parameters, which slows down training. Fortunately, new generations of training hardware as well as software optimizations, make it a feasible task. \n",
    "\n",
    "However, where most of the (both hardware and software) optimization opportunities exists is in exploiting lower precision (like FP16) to, for example, utilize Tensor Cores available on new Volta and Turing GPUs. While training in FP16 showed great success in image classification tasks, other more complicated neural networks typically stayed in FP32 due to difficulties in applying the FP16 training guidelines.\n",
    "\n",
    "That is where AMP (Automatic Mixed Precision) comes into play. It automatically applies the guidelines of FP16 training, using FP16 precision where it provides the most benefit, while conservatively keeping in full FP32 precision operations unsafe to do in FP16.\n",
    "\n",
    "This tutorial shows how to get started with mixed precision training using AMP for MXNet. As an example of a network we will use SSD network from GluonCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader and helper functions\n",
    "\n",
    "For demonstration purposes we will use synthetic data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.gluon as gluon\n",
    "import gluoncv as gcv\n",
    "\n",
    "data_shape = 512\n",
    "batch_size = 8\n",
    "\n",
    "# set up logger\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Start training')\n",
    "\n",
    "ce_metric = mx.metric.Loss('CrossEntropy')\n",
    "smoothl1_metric = mx.metric.Loss('SmoothL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataLoader(object):\n",
    "    def __init__(data_shape, batch_size):\n",
    "        super(SyntheticDataLoader, self).__init__()\n",
    "        self.counter = 0\n",
    "        self.epoch_size = 200\n",
    "        self.data = None\n",
    "        self.cls_targets = None\n",
    "        self.box_targets = None\n",
    "    \n",
    "    def __next__():\n",
    "        if self.counter >= self.epoch_size:\n",
    "            self.counter = self.counter % self.epoch_size\n",
    "            raise StopIteration\n",
    "        self.counter += 1\n",
    "        return [self.data, self.cls_targets, self.box_targets]\n",
    "    \n",
    "train_data = SyntheticDataLoader(data_shape, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network():\n",
    "    # SSD with RN50 backbone\n",
    "\n",
    "    # training contexts\n",
    "    ctx = [mx.gpu(0)]\n",
    "\n",
    "    # network\n",
    "    net_name = 'ssd_512_resnet50_v1_coco'\n",
    "    net = get_model(net_name, pretrained_base=True, norm_layer=gluon.nn.BatchNorm)\n",
    "    async_net = net\n",
    "    \n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_network()\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "\n",
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': args.lr, 'wd': args.wd, 'momentum': args.momentum})\n",
    "\n",
    "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "            cls_preds = []\n",
    "            box_preds = []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                cls_preds, box_preds, cls_targets, box_targets)\n",
    "            autograd.backward(sum_loss)\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        if not (i + 1) % 50:\n",
    "            name1, loss1 = ce_metric.get()\n",
    "            name2, loss2 = smoothl1_metric.get()\n",
    "            logger.info('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()\n",
    "\n",
    "    name1, loss1 = ce_metric.get()\n",
    "    name2, loss2 = smoothl1_metric.get()\n",
    "    logger.info('[Epoch {}] Training cost: {:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "        epoch, (time.time()-tic), name1, loss1, name2, loss2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with AMP\n",
    "\n",
    "### AMP initialization\n",
    "\n",
    "In order to start using AMP, we need to import and initialize it. This has to happen before we create the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import amp\n",
    "\n",
    "amp.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can create the network exactly the same way we did in FP32 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_network()\n",
    "net.hybridize(static_alloc=True, static_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some models that may be enough to start training in mixed precision, but the full FP16 recipe recommends using dynamic loss scaling to guard against over- and underflows of FP16 values. Therefore, as a next step, we create a trainer and initialize it with support for AMP's dynamic loss scaling. Currently, support for dynamic loss scaling is limited to trainers created with `update_on_kvstore=False` option, and so we add it to our trainer initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(\n",
    "    net.collect_params(), 'sgd',\n",
    "    {'learning_rate': args.lr, 'wd': args.wd, 'momentum': args.momentum},\n",
    "    update_on_kvstore=False)\n",
    "\n",
    "amp.init_trainer(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic loss scaling in the training loop\n",
    "\n",
    "The last step is to apply the dynamic loss scaling during the training loop and . We can achieve that using the `amp.scale_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox_loss = gcv.loss.SSDMultiBoxLoss()\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    ce_metric.reset()\n",
    "    smoothl1_metric.reset()\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_data):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        cls_targets = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        box_targets = gluon.utils.split_and_load(batch[2], ctx_list=ctx, batch_axis=0)\n",
    "        with autograd.record():\n",
    "            cls_preds = []\n",
    "            box_preds = []\n",
    "            for x in data:\n",
    "                cls_pred, box_pred, _ = net(x)\n",
    "                cls_preds.append(cls_pred)\n",
    "                box_preds.append(box_pred)\n",
    "            sum_loss, cls_loss, box_loss = mbox_loss(\n",
    "                cls_preds, box_preds, cls_targets, box_targets)\n",
    "            with amp.scale_loss(sum_loss, trainer) as scaled_loss:\n",
    "                autograd.backward(scaled_loss)\n",
    "        trainer.step(1)\n",
    "        ce_metric.update(0, [l * batch_size for l in cls_loss])\n",
    "        smoothl1_metric.update(0, [l * batch_size for l in box_loss])\n",
    "        if not (i + 1) % 50:\n",
    "            name1, loss1 = ce_metric.get()\n",
    "            name2, loss2 = smoothl1_metric.get()\n",
    "            logger.info('[Epoch {}][Batch {}], Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, batch_size/(time.time()-btic), name1, loss1, name2, loss2))\n",
    "        btic = time.time()\n",
    "\n",
    "    name1, loss1 = ce_metric.get()\n",
    "    name2, loss2 = smoothl1_metric.get()\n",
    "    logger.info('[Epoch {}] Training cost: {:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "        epoch, (time.time()-tic), name1, loss1, name2, loss2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
